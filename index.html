<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Language Models of Code Are Few-Shot Planners and Reasoners for Multi-Document Summarization with Attribution">
  <meta property="og:title" content="YesBut Dataset - Satire Comprehension with Vision-Language Models"/>
  <meta property="og:description" content="YesBut introduces tasks for Satirical Image Detection, Understanding, and Completion, along with a dataset for evaluating Vision-Language models."/>
  <meta property="og:url" content="https://github.com/abhi1nandy2/yesbut_dataset"/>
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="YesBut Dataset: Satire Comprehension in Vision-Language Models">
  <meta name="twitter:description" content="A dataset of satirical and non-satirical images for evaluating Vision-Language models, with challenging tasks like Satirical Image Detection, Understanding, and Completion.">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="satire, dataset, vision-language models, satire comprehension, humor detection, multimodal datasets">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>YesBut Dataset - Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Language Models of Code Are Few-Shot Planners and Reasoners for Multi-Document Summarization with Attribution</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://sites.google.com/view/abhilashnandy" target="_blank">Abhilash Nandy</a>,</span>
              
              <span class="author-block">
                <a href="https://research.adobe.com/person/sambaran-bandyopadhyay/" target="_blank">Sambaran Bandyopadhyay</a>              
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Indian Institute of Technology Kharagpur, Adobe Research</span><br>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <h3 class="title is-3">AAAI 2025</h3>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- ArXiv abstract Link -->
<span class="link-block">
  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/34676" target="_blank" class="external-link button is-normal is-rounded is-dark">
    <span class="icon">
      <i class="fas fa-file-alt" style="font-size:20px;"></i>
    </span>
    <span>Paper</span>
  </a>
</span>
                
<span class="link-block">
  <a href="https://drive.google.com/file/d/1lWqQtHRnpn-g2IQ3guloj_2j8sDm4z0V/view?usp=drivesdk" target="_blank" class="external-link button is-normal is-rounded is-dark">
    <span class="icon">
      <i class="fas fa-chalkboard" style="font-size:20px;"></i>
    </span>
    <span>Slides</span>
  </a>
</span>
<span class="link-block">
  <a href="https://drive.google.com/file/d/1EQqgwbcS7xkVx38y0qPvdRh0gQyeOH5u/view?usp=drivesdk" target="_blank" class="external-link button is-normal is-rounded is-dark">
    <span class="icon">
      <i class="fas fa-image" style="font-size:20px;"></i>
    </span>
    <span>Poster</span>
  </a>
</span>
       

                <span class="link-block">
                  <a href="https://youtube.com/shorts/6ecxLLUpWJE?si=x1C53Y-JqJeV5ftW" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <p style="font-size:20px">▶</p>
                    </span>
                    <span>YouTube</span>
                  </a>
                </span>                                

                
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser video -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- Twitter/X Embed 
        <div style="display: flex; justify-content: center;">
        <iframe border=0 frameborder=0 height=380 width=500 src="https://twitframe.com/show?url=https%3A%2F%2Ftwitter.com%2Fabhilash_nandy%2Fstatus%2F1837183220820943286"></iframe>
        </div>
        -->

  
        <!-- Subtitle -->
        <h2 class="subtitle has-text-centered">
          We tackle the challenge of generating coherent, non-redundant summaries from heterogeneous document collections with source attribution by introducing     ✨ <span style="color: #FFD700;"><b><i>MiDAS</i></b></span>-<span style="color: #FFC0CB;"><b><i>PRo</i></b></span> ✨, a three-stage LLM-based pipeline—(1) planning a hierarchical document organization, (2) reasoning via entity/topic generation, and (3) final summary creation—where aforementioned planning and reasoning are framed as code-completion tasks enhanced by graph-attention–guided in-context example selection.
        </h2>
      </div>
    </div>
  
    <!-- Ensure that the X embed script is loaded -->
    <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
  </section>
  

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Document summarization has greatly benefited from advances in large language models (LLMs). In real-world situations, summaries often need to be generated from multiple documents with diverse sources and authors, lacking a clear information flow. Naively concatenating these documents and generating a summary can lead to poorly structured narratives and redundancy. Additionally, attributing each part of the generated summary to a specific source is crucial for reliability. In this study, we address multi-document summarization with attribution using our proposed solution ✨ <span style="color: #FFD700;"><b><i>MiDAS</i></b></span>-<span style="color: #FFC0CB;"><b><i>PRo</i></b></span> ✨ (Multi-Document Attribution-inclusive Summarization via Planning-cum-Reasoning) consisting of three stages: (i) Planning the hierarchical organization of source documents, (ii) Reasoning by generating relevant entities/topics, and (iii) Summary Generation. We treat the first two sub-problems as a code completion task for LLMs. By incorporating well-selected in-context learning examples through a graph attention network, LLMs effectively generate plans and reason topics for a document collection. Experiments on summarizing scientific articles from public datasets show that our approach outperforms state-of-the-art baselines in both automated and human evaluations.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">


      <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           A Dataset Sample
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
         This Figure shows an example image conveying satire. The irony in the image is that the person is messaging someone a very heartfelt message on the mobile, while sitting on a toilet seat!
           <br><br>
           <img src="static/images/MiDAS-PRo.jpg"  alt="MY ALT TEXT" style="width: 50%; display: block; margin: auto;"/>
         </p>
         </h3>
      </div>

      <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           YesBut Dataset Curation
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
          Our annotation Pipeline for YesBut in 4 Stages- (1) Collecting Satirical Images from Social Media (2) Human Annotation of satirical images (3) Generating 2D stick images using DALL-E 3 and annotated descriptions (4) Generating 3D stick images using DALL-E 3 and annotated descriptions
         <br><br>
           <img src="static/images/gat.jpg"  alt="MY ALT TEXT" style="width: 50%; display: block; margin: auto;"/>
         </p>
         </h3>
      </div>

      <div class="item">
        <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
          Distribution of the YesBut Dataset
          </h2>
        <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
        <p>
          Distribution of the original 283 satirical images downloaded from Social Media based on different aspects of image content and annotated descriptions
        <br><br>
          <img src="static/images/1-shot-multixscience.jpg"  alt="MY ALT TEXT" style="display: block; margin: auto;"/>
        </p>
        </h3>
     </div>

      <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Task - Satirical Image Detection
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
          This is a binary classification task, where given an image, the model needs to predict whether the image is satirical or not.
         <br><br>
           <img src="static/images/Midas-dataset.jpg"  alt="MY ALT TEXT" style="display: block; margin: auto;"/>
         </p>
         </h3>
      </div>



      <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Task - Satirical Image Understanding
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
          Given a satirical image, we evaluate the model’s satire understanding capability in images by (1) prompting the model to generate a textual description of each subimage as input, using the prompt “Describe the image”. (2) prompting the model to generate the punchline in the image using the following prompt (referred to as “WHYFUNNY_PROMPT” hereafter)- “Why is this image funny/satirical?”.
         <br><br>
           <img src="static/images/k-shot.jpg"  alt="MY ALT TEXT" style="display: block; margin: auto;"/>
         </p>
         </h3>
      </div>

      <div class="item">
        <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
          Task - Satirical Image Completion
          </h2>
        <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
        <p>
          Given either the left or right sub-image having the style of a colorized sketch, the other sub-image needs to be chosen from two options, one having a 2D, and the other having a 3D stick figure style, such that the entire image so formed is meaningful and satirical.
        <br><br>
          <img src="static/images/Ablation.jpg"  alt="MY ALT TEXT" style="display: block; margin: auto;"/>
        </p>
        </h3>
     </div>      

     <div class="item">
      <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
        Results
        </h2>
      <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
      <p>
       
        <img src="static/images/Baselines.jpg"  alt="MY ALT TEXT" style="display: block; margin: auto;"/>
      </p>
      </h3>
   </div>           

     <div class="item">
      <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
        Results
        </h2>
      <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
      <p>
       
        <img src="static/images/Human-eval.jpg"  alt="MY ALT TEXT" style="display: block; margin: auto;"/>
      </p>
      </h3>
   </div>          

<!--      <div class="item">-->
<!--         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">-->
<!--           Using Weird Images to Create V&L Tasks-->
<!--           </h2>-->
<!--         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">-->
<!--         <p>-->
<!--         The WHOOPS! benchmark includes four tasks:-->
<!--            <ol class="pr-2 pl-6">-->
<!--              <li>A novel task of explanation-of-violation: generating a detailed explanation for what makes the image weird</li>-->
<!--              <li>Generating a literal caption</li>-->
<!--              <li>Distinguishing between detailed and underspecified captions</li>-->
<!--             <li>Answering questions that test compositional understanding</li>-->
<!--            </ol>-->
<!--             <br><br>-->
<!--           <img src="static/images/benchmarking.png"/>-->
<!--         </p>-->
<!--         </h3>-->
<!--      </div>-->

<!--      <div class="item">-->
<!--         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">-->
<!--           Test Results for Explanation-of-violation-->
<!--           </h2>-->
<!--         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">-->
<!--           <p>Models significantly lag behind human performance. For example, on identification, the best end-to-end fine-tuned BLIP2 FlanT5-XXL model achieves at best 73%. For explanation, even the oracle model (which is given access to a ground-truth, human-authored description of the image) only achieves a performance of 68%, falling substantially short of human performance (95%). We also added auto-eval results that are correlated with the human-eval. These results indicate that our dataset provides a challenging benchmark for the development of next-generation vision-and-language models.</p>-->
<!--         <p style="text-align:center;">-->
<!--           <br><br>-->
<!--           <img src="static/images/Tb1.png"  style="width: 80%; height: 80%"/>-->
<!--         </p>-->
<!--         </h3>-->
<!--      </div>-->

<!--      <div class="item">-->
<!--        <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">-->
<!--           Test Results for Image Captioning, Cross-Modal Matching and Visual Question Answering-->
<!--           </h2>-->
<!--         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">-->
<!--           <p>The zero-shot results highlight the strengths and weaknesses of each model. Zero-shot BLIP2 demonstrates a substantial improvement over the other models. But even the supervised models have significant room for improvement, especially in VQA (maximum BEM score is 57%) and image captioning-->
<!--           </p>-->
<!--         <p style="text-align:center;">-->
<!--           <br><br>-->
<!--           <img src="static/images/Tb2.png"  style="width: 80%; height: 80%"/>-->
<!--         </p>-->
<!--         </h3>-->
<!--      </div>-->
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube Shorts video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube Shorts embed code -->
            <iframe src="https://www.youtube.com/embed/6ecxLLUpWJE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Youtube Shorts video -->




<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="https://drive.google.com/file/d/1EQqgwbcS7xkVx38y0qPvdRh0gQyeOH5u/preview" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>


<!-- Paper -->

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Paper</h2>

      <iframe  src="https://drive.google.com/file/d/1qVGG2jaVDqQOhxoMwSsuXFEqqKlifHtH/preview" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{Nandy_Bandyopadhyay_2025, title={Language Models of Code Are Few-Shot Planners and Reasoners for Multi-Document Summarization with Attribution}, volume={39}, url={https://ojs.aaai.org/index.php/AAAI/article/view/34676}, 
      DOI={10.1609/aaai.v39i23.34676},
      abstractNote={Document summarization has greatly benefited from advances in large language models (LLMs). In real-world situations, summaries often need to be generated from multiple documents with diverse sources and authors, lacking a clear information flow. Naively concatenating these documents and generating a summary can lead to poorly structured narratives and redundancy. Additionally, attributing each part of the generated summary to a specific source is crucial for reliability. In this study, we address multi-document summarization with attribution using our proposed solution ***MiDAS-PRo***, consisting of three stages: (i) Planning the hierarchical organization of source documents, (ii) Reasoning by generating relevant entities/topics, and (iii) Summary Generation. We treat the first two sub-problems as a code completion task for LLMs. By incorporating well-selected in-context learning examples through a graph attention network, LLMs effectively generate plans and reason topics for a document collection. Experiments on summarizing scientific articles from public datasets show that our approach outperforms state-of-the-art baselines in both automated and human evaluations.}, 
      number={23}, 
      journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
      author={Nandy, Abhilash and Bandyopadhyay, Sambaran}, 
      year={2025}, 
      month={Apr.}, 
      pages={24930-24938} 
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
